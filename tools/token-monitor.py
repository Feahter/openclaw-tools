#!/usr/bin/env python3
"""
Token æ¶ˆè€—ç›‘æ§å™¨
è®°å½• API è°ƒç”¨ï¼Œç»Ÿè®¡ token æ¶ˆè€—ï¼Œç›‘æ§ä¸Šä¸‹æ–‡å¤§å°
"""

import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict

# é…ç½®
WORKSPACE = "/Users/fuzhuo/.openclaw/workspace"
DATA_DIR = Path(WORKSPACE) / "data"
TOKEN_LOG_FILE = DATA_DIR / "token-usage.json"
SESSION_DIR = DATA_DIR / "sessions"
MAX_CONTEXT_TOKENS = 100000  # ä¸Šä¸‹æ–‡è¿‡å¤§é˜ˆå€¼
MAX_HISTORY_MESSAGES = 20    # æœ€å¤§å†å²æ¶ˆæ¯æ•°

DATA_DIR.mkdir(parents=True, exist_ok=True)
SESSION_DIR.mkdir(parents=True, exist_ok=True)

def init_log():
    """åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶"""
    if not TOKEN_LOG_FILE.exists():
        TOKEN_LOG_FILE.write_text(json.dumps({"records": [], "daily_stats": {}}, indent=2))

def log_usage(provider: str, model: str, prompt_tokens: int, completion_tokens: int, 
              cost: float, session_key: str = None, message_type: str = "chat"):
    """è®°å½•ä¸€æ¬¡ API è°ƒç”¨"""
    now = datetime.now()
    date_key = now.strftime("%Y-%m-%d")
    hour_key = now.strftime("%Y-%m-%d %H:00")
    
    record = {
        "timestamp": now.isoformat(),
        "date": date_key,
        "hour": hour_key,
        "provider": provider,
        "model": model,
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
        "total_tokens": prompt_tokens + completion_tokens,
        "cost": cost,
        "session_key": session_key,
        "message_type": message_type
    }
    
    # è¯»å–ç°æœ‰æ•°æ®
    data = json.loads(TOKEN_LOG_FILE.read_text())
    
    # æ·»åŠ è®°å½•
    data["records"].append(record)
    
    # æ›´æ–°æ¯æ—¥ç»Ÿè®¡
    if date_key not in data["daily_stats"]:
        data["daily_stats"][date_key] = {"total_tokens": 0, "cost": 0, "requests": 0, "by_provider": {}}
    
    stats = data["daily_stats"][date_key]
    stats["total_tokens"] += record["total_tokens"]
    stats["cost"] += cost
    stats["requests"] += 1
    
    if provider not in stats["by_provider"]:
        stats["by_provider"][provider] = {"tokens": 0, "cost": 0, "requests": 0}
    
    pstats = stats["by_provider"][provider]
    pstats["tokens"] += record["total_tokens"]
    pstats["cost"] += cost
    pstats["requests"] += 1
    
    # åªä¿ç•™æœ€è¿‘ 30 å¤©çš„æ•°æ®
    cutoff = (now - timedelta(days=30)).strftime("%Y-%m-%d")
    data["daily_stats"] = {k: v for k, v in data["daily_stats"].items() if k >= cutoff}
    data["records"] = [r for r in data["records"] if r["date"] >= cutoff]
    
    # å†™å…¥æ–‡ä»¶
    TOKEN_LOG_FILE.write_text(json.dumps(data, indent=2, ensure_ascii=False))
    
    return record

def get_daily_stats(date: str = None):
    """è·å–æ¯æ—¥ç»Ÿè®¡"""
    data = json.loads(TOKEN_LOG_FILE.read_text())
    if date is None:
        date = datetime.now().strftime("%Y-%m-%d")
    return data["daily_stats"].get(date, {"total_tokens": 0, "cost": 0, "requests": 0})

def get_recent_usage(hours: int = 24):
    """è·å–æœ€è¿‘ N å°æ—¶çš„æ¶ˆè€—"""
    cutoff = datetime.now() - timedelta(hours=hours)
    data = json.loads(TOKEN_LOG_FILE.read_text())
    
    recent = [r for r in data["records"] if datetime.fromisoformat(r["timestamp"]) > cutoff]
    
    total_prompt = sum(r["prompt_tokens"] for r in recent)
    total_completion = sum(r["completion_tokens"] for r in recent)
    total_cost = sum(r["cost"] for r in recent)
    
    by_provider = defaultdict(lambda: {"prompt": 0, "completion": 0, "cost": 0})
    for r in recent:
        by_provider[r["provider"]]["prompt"] += r["prompt_tokens"]
        by_provider[r["provider"]]["completion"] += r["completion_tokens"]
        by_provider[r["provider"]]["cost"] += r["cost"]
    
    return {
        "period_hours": hours,
        "total_prompt_tokens": total_prompt,
        "total_completion_tokens": total_completion,
        "total_tokens": total_prompt + total_completion,
        "total_cost": total_cost,
        "by_provider": dict(by_provider),
        "records": recent[-50:]  # æœ€è¿‘ 50 æ¡
    }

def check_context_size(messages: list, session_key: str = None) -> dict:
    """æ£€æŸ¥ä¸Šä¸‹æ–‡å¤§å°ï¼Œè¿”å›çŠ¶æ€å’Œå»ºè®®"""
    # ç®€å•ä¼°ç®—ï¼šå¹³å‡æ¯ä¸ª token çº¦ 4 å­—ç¬¦
    content = json.dumps(messages)
    estimated_tokens = len(content) // 4
    
    status = "ok"
    actions = []
    
    if estimated_tokens > MAX_CONTEXT_TOKENS:
        status = "warning"
        actions.append(f"âš ï¸ ä¸Šä¸‹æ–‡è¿‡å¤§ ({estimated_tokens:,} tokens)")
        actions.append(f"å»ºè®®: å‡å°‘å†å²æ¶ˆæ¯è‡³ {MAX_HISTORY_MESSAGES} æ¡ä»¥å†…")
    
    if len(messages) > MAX_HISTORY_MESSAGES:
        status = "warning"
        actions.append(f"ğŸ“ å†å²æ¶ˆæ¯è¿‡å¤š ({len(messages)} æ¡)")
        actions.append(f"å»ºè®®: ä¿ç•™æœ€è¿‘ {MAX_HISTORY_MESSAGES} æ¡ï¼Œå‹ç¼©æ—©æœŸå†…å®¹")
    
    return {
        "estimated_tokens": estimated_tokens,
        "message_count": len(messages),
        "status": status,
        "actions": actions,
        "session_key": session_key
    }

def optimize_history(messages: list, keep_last: int = MAX_HISTORY_MESSAGES) -> list:
    """ä¼˜åŒ–å†å²æ¶ˆæ¯ï¼Œè‡ªåŠ¨å‡è´Ÿ"""
    if len(messages) <= keep_last:
        return messages
    
    # ä¿ç•™ç³»ç»Ÿæç¤º + æœ€è¿‘ N æ¡
    optimized = messages[:1] + messages[-keep_last+1:] if messages else messages
    
    # å¦‚æœç¬¬ä¸€æ¡æ˜¯ç³»ç»Ÿæ¶ˆæ¯ï¼Œä¿ç•™ï¼›å¦åˆ™ä»åå¾€å‰ä¿ç•™
    if not optimized or optimized[0].get("role") != "system":
        optimized = messages[-keep_last:]
    
    return optimized

def save_session_history(session_key: str, messages: list, token_count: int = 0):
    """ä¿å­˜ä¼šè¯å†å²"""
    session_file = SESSION_DIR / f"{session_key}.json"
    session_data = {
        "session_key": session_key,
        "saved_at": datetime.now().isoformat(),
        "message_count": len(messages),
        "estimated_tokens": token_count,
        "messages": messages[-50:]  # åªä¿å­˜æœ€è¿‘ 50 æ¡
    }
    session_file.write_text(json.dumps(session_data, indent=2, ensure_ascii=False))

def get_session_history(session_key: str) -> list:
    """è·å–ä¼šè¯å†å²"""
    session_file = SESSION_DIR / f"{session_key}.json"
    if session_file.exists():
        data = json.loads(session_file.read_text())
        return data.get("messages", [])
    return []

def print_usage_report(usage: dict):
    """æ‰“å°æ¶ˆè€—æŠ¥å‘Š"""
    print(f"\nğŸ“Š Token æ¶ˆè€—æŠ¥å‘Š (æœ€è¿‘ {usage['period_hours']} å°æ—¶)")
    print("=" * 50)
    print(f"æ€»æ¶ˆè€— Tokens: {usage['total_tokens']:,}")
    print(f"  - Prompt: {usage['total_prompt_tokens']:,}")
    print(f"  - Completion: {usage['total_completion_tokens']:,}")
    print(f"æ€»è´¹ç”¨: ${usage['total_cost']:.4f}")
    print("\næŒ‰ Provider:")
    for provider, stats in usage["by_provider"].items():
        print(f"  {provider}: {stats['prompt']+stats['completion']:,} tokens, ${stats['cost']:.4f}")
    print(f"\næœ€è¿‘ {len(usage['records'])} æ¡è®°å½•")

def main():
    import sys
    
    init_log()
    
    if len(sys.argv) < 2:
        print("Token Monitor - Token æ¶ˆè€—ç›‘æ§")
        print("\nç”¨æ³•:")
        print("  python3 token-monitor.py log <provider> <model> <prompt> <completion> [cost] [session]")
        print("  python3 token-monitor.py daily          # ä»Šæ—¥ç»Ÿè®¡")
        print("  python3 token-monitor.py recent [hours] # æœ€è¿‘æ¶ˆè€—")
        print("  python3 token-monitor.py check          # æ£€æŸ¥å½“å‰ä¼šè¯")
        print("  python3 token-monitor.py optimize <file> # ä¼˜åŒ–å†å²æ–‡ä»¶")
        sys.exit(0)
    
    cmd = sys.argv[1]
    
    if cmd == "daily":
        stats = get_daily_stats()
        print(f"\nğŸ“Š ä»Šæ—¥ç»Ÿè®¡")
        print(f"æ€» Tokens: {stats['total_tokens']:,}")
        print(f"æ€»è¯·æ±‚: {stats['requests']}")
        print(f"æ€»è´¹ç”¨: ${stats['cost']:.4f}")
        for provider, ps in stats.get("by_provider", {}).items():
            print(f"  {provider}: {ps['tokens']:,} tokens, ${ps['cost']:.4f}")
    
    elif cmd == "recent":
        hours = int(sys.argv[2]) if len(sys.argv) > 2 else 24
        usage = get_recent_usage(hours)
        print_usage_report(usage)
    
    elif cmd == "log":
        # python3 token-monitor.py log <provider> <model> <prompt_tokens> <completion_tokens> [cost] [session]
        provider = sys.argv[2]
        model = sys.argv[3]
        prompt = int(sys.argv[4])
        completion = int(sys.argv[5])
        cost = float(sys.argv[6]) if len(sys.argv) > 6 else 0
        session = sys.argv[7] if len(sys.argv) > 7 else None
        
        record = log_usage(provider, model, prompt, completion, cost, session)
        print(f"âœ… å·²è®°å½•: {provider}/{model} - {record['total_tokens']:,} tokens")
    
    elif cmd == "check":
        # æ£€æŸ¥å½“å‰ä¼šè¯å¤§å°
        print("\nğŸ” æ£€æŸ¥ä¼šè¯ä¸Šä¸‹æ–‡å¤§å°...")
        print(f"é˜ˆå€¼: {MAX_CONTEXT_TOKENS:,} tokens")
        print(f"æœ€å¤§å†å²æ¶ˆæ¯: {MAX_HISTORY_MESSAGES} æ¡")
        
        # æ£€æŸ¥æ‰€æœ‰ session æ–‡ä»¶
        total_tokens = 0
        session_count = 0
        for f in SESSION_DIR.glob("*.json"):
            data = json.loads(f.read_text())
            tokens = data.get("estimated_tokens", 0)
            total_tokens += tokens
            session_count += 1
            if tokens > MAX_CONTEXT_TOKENS:
                print(f"  âš ï¸ {f.name}: {tokens:,} tokens")
        
        print(f"\næ€»è®¡: {session_count} ä¸ªä¼šè¯, çº¦ {total_tokens:,} tokens")
    
    elif cmd == "optimize":
        if len(sys.argv) > 2:
            file_path = sys.argv[2]
            if os.path.exists(file_path):
                messages = json.loads(open(file_path).read())
                optimized = optimize_history(messages)
                print(f"åŸå§‹: {len(messages)} æ¡")
                print(f"ä¼˜åŒ–å: {len(optimized)} æ¡")
                # è¾“å‡ºåˆ° stdout
                print("\n" + json.dumps(optimized, ensure_ascii=False, indent=2))
            else:
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        else:
            print("ç”¨æ³•: python3 token-monitor.py optimize <file>")
    
    else:
        print(f"æœªçŸ¥å‘½ä»¤: {cmd}")
        print("ç”¨ python3 token-monitor.py æŸ¥çœ‹å¸®åŠ©")

if __name__ == "__main__":
    main()
